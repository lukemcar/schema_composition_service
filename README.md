# My Entity Starter Python (`my_entity_starter_python`)

`my_entity_starter_python` is a **production‑grade Python microservice starter template** designed to act as a gold‑standard scaffold for building new, tenant‑aware, event‑driven services.  The repository implements exactly **one domain (`MyEntity`)** but includes **every architectural layer you would expect in a real service**: FastAPI API, SQLAlchemy models, Liquibase migrations, messaging via Celery/RabbitMQ, background workers, OpenTelemetry instrumentation, comprehensive tests, and container orchestration with Docker.  The goal of this repository is to provide clarity and correctness—not feature breadth—so that other services can be derived from it without guessing about patterns or behaviour.

This README is intentionally **verbose** because it is both a teaching artifact for humans and an input for AI agents that will generate new services from this starter.  Every file and directory is listed and described; non‑negotiable patterns are called out explicitly.  You should treat this document as the authoritative reference for how the service works and how to extend it.

---

## Purpose of this document

This document serves two critical purposes:

1. **Human reference guide.** Developers can read through the repository structure, understand the responsibilities of each component and follow established patterns when adding new domains.  The document emphasises why certain files exist, how they interact, and what must be preserved when copying the scaffold.
2. **Machine‑ingestable guide.** Future AI agents will use this README to generate new microservices from the template.  Deterministic behaviour is paramount: two agents given the same input should produce materially identical output.  Consequently, this README avoids speculation, follows consistent language, and describes behaviour precisely.

Throughout this document you will see notes indicating when something is **boilerplate** (must be copied verbatim) versus when something is **domain‑specific** (must be duplicated and renamed when adding a new domain).  Unless otherwise stated, assume that file names and identifiers containing `my_entity` must be changed when creating a new domain, and everything else should remain unchanged.

---

## Repository structure

The repository is organised into top‑level directories for source code (`app`), configuration (`docker` and `migrations`), packaging metadata, design artefacts and tests.  Hidden directories such as `.git`, `.venv` and `.pytest_cache` are not relevant to the service itself and are omitted from the following tree for brevity.  Generated bytecode directories (`__pycache__`) are similarly omitted.  The remaining directories and files are enumerated to full path depth:

```
.
├── Makefile
├── README.md                  # This document
├── main_api.py                # Entry point for the FastAPI application
├── pyproject.toml             # Python project metadata and dependencies
├── pytest.ini                 # Pytest configuration
├── docker-compose.yml         # Docker orchestration for development and deployment
├── docker-compose.test.yml    # Docker orchestration for integration tests
├── .env.test                  # Sample environment overrides for tests (not committed in prod)
├── .gitignore                 # Git ignore rules
├── app/                       # All service source code lives here
│   ├── ai/
│   │   └── agents/
│   │       ├── __init__.py
│   │       └── simple_agent.py
│   ├── api/
│   │   ├── __init__.py
│   │   ├── error_handlers.py
│   │   └── routes/
│   │       ├── __init__.py
│   │       ├── health.py
│   │       └── my_entity.py
│   ├── core/
│   │   ├── celery_app.py
│   │   ├── config.py
│   │   ├── db.py
│   │   ├── logging.py
│   │   └── telemetry.py
│   ├── domain/
│   │   ├── models/
│   │   │   ├── __init__.py
│   │   │   ├── base.py
│   │   │   └── my_entity.py
│   │   ├── schemas/
│   │   │   ├── __init__.py
│   │   │   ├── common.py
│   │   │   ├── health.py
│   │   │   ├── json_patch.py
│   │   │   ├── my_entity.py
│   │   │   └── events/
│   │   │       ├── __init__.py
│   │   │       ├── common.py
│   │   │       └── my_entity_events.py
│   │   └── services/
│   │       ├── __init__.py
│   │       ├── health_service.py
│   │       └── my_entity_service.py
│   ├── messaging/
│   │   ├── producers/
│   │   │   └── my_entity_producer.py
│   │   └── tasks/
│   │       ├── __init__.py
│   │       └── my_entity_tasks.py
│   ├── util/
│   │   ├── __init__.py
│   │   ├── correlation.py
│   │   ├── jwt_util.py
│   │   └── liquibase.py
│   └── formless_agent_service.egg-info/
│       ├── PKG-INFO
│       ├── SOURCES.txt
│       ├── dependency_links.txt
│       ├── requires.txt
│       └── top_level.txt
├── design/                    # Reserved for design docs; empty in this starter
├── docker/
│   ├── api/Dockerfile
│   ├── worker/Dockerfile
│   ├── otel/otel-collector-config.yml
│   └── postgres/init-database.sql
├── migrations/
│   └── liquibase/
│       ├── changelog-root.xml
│       ├── docker-liquibase.properties
│       ├── liquibase.properties
│       ├── test-liquibase.properties
│       ├── postgresql-42.5.4.jar
│       └── sql/
│           └── 001_create_my_entity.sql
└── tests/
    ├── conftest.py
    ├── test_otel_collector_config.py
    ├── test_telemetry_resource_merge.py
    ├── ai/
    │   └── test_simple_agent.py
    ├── api/
    │   ├── test_health.py
    │   └── test_my_entity.py
    └── domain/
        └── services/
            └── test_my_entity_service.py
```

Generated caches (`__pycache__`), local virtual environments (`.venv`) and internal Git metadata (`.git`) exist in the repository but are not part of the template itself; they are ignored for clarity and should not be checked into version control when copying the starter to a new project.

---

## Domain implementation: **MyEntity**

The `MyEntity` domain demonstrates how to implement a tenant‑scoped resource throughout every layer of the service.  When adding your own domain, copy the patterns used by `MyEntity` and adjust the names and schema to suit your new object.  The following subsections describe how each layer works and how the pieces fit together.

### `app/api/routes`

The `app/api/routes` package contains FastAPI routers for different resources.  `health.py` defines liveness and readiness probes which can be used by orchestrators such as Kubernetes to monitor the service.  `my_entity.py` defines the REST API for the `MyEntity` resource.  All routes are mounted under a tenant prefix (`/tenants/{tenant_id}/my-entities`) to enforce tenant scoping: the path parameter `tenant_id` is validated against the `tenant_id` claim in the JWT token by the `auth_jwt` dependency.  The router exposes endpoints to list entities with pagination (`GET /`), create a new entity (`POST /`), retrieve a single entity (`GET /{my_entity_id}`), replace an entity (`PUT /{my_entity_id}`), delete an entity (`DELETE /{my_entity_id}`) and apply a JSON Patch (`PATCH /{my_entity_id}`).  Each endpoint delegates to the service layer and returns Pydantic models defined in `app/domain/schemas`; no business logic is embedded in the routes themselves.  When adding a new domain router, copy `my_entity.py`, change the prefix, and swap out the schema and service references.

### `app/domain/models`

Domain models define the database tables using SQLAlchemy.  `base.py` provides a `DeclarativeBase` called `Base` with a naming convention for constraints; this base class is used for all models.  `my_entity.py` defines the `MyEntity` model with a UUID primary key (`my_entity_id`), tenant identifier (`tenant_id`), a name, an arbitrary JSON payload stored in a `JSONB` column (`data`), audit timestamps (`created_at` and `updated_at`) and audit user fields (`created_by` and `updated_by`).  The constructor uses `uuid4` and `datetime.utcnow` defaults so that values are generated on the server.  All models should inherit from `Base` and avoid calling `metadata.create_all`; the schema is managed exclusively by Liquibase migrations.  When extending the service you will add additional model classes here that mirror the table definitions in your SQL migration.

### `app/domain/schemas`

Pydantic schemas describe the shape of data entering and leaving the API.  `common.py` provides a reusable `ErrorResponseBody` for consistent error payloads and a generic `PaginationEnvelope[T]` for paginated responses.  `health.py` defines a simple `HealthResponse` model used by the health probes.  `json_patch.py` contains a detailed schema for JSON Patch requests along with helpers for validating operations; it defines `JsonPatchOperation` and `JsonPatchRequest`, enforcing RFC 6902 semantics and enumerating allowed paths for `MyEntity` (`/name` and `/data`).  `my_entity.py` defines the request and response models for the `MyEntity` API: `MyEntityCreate` includes the required fields for creation, `MyEntityUpdate` allows partial updates via PUT, `MyEntityOut` is used for responses and reads values directly from SQLAlchemy models (`ConfigDict.from_attributes=True`), and `MyEntityListResponse` wraps a list of entities in a pagination envelope.  Under the nested `events` package, `common.py` defines a generic `EventEnvelope` used for all messages and `my_entity_events.py` defines `MyEntityCreatedMessage`, `MyEntityUpdatedMessage` and `MyEntityDeletedMessage` to describe the payloads published on the message bus.  When adding a new domain, you will duplicate the `my_entity` schemas and adjust the fields accordingly; the common schemas remain unchanged.

### `app/domain/services`

The service layer encapsulates business logic and database transactions.  `health_service.py` performs liveness and readiness checks by optionally verifying the database connection and returns a `HealthResponse`.  `my_entity_service.py` contains functions to create, retrieve, list, update, patch and delete `MyEntity` records.  Each function accepts an explicit SQLAlchemy session (`db`) and the `tenant_id` to enforce tenant boundaries.  On creation and update, audit fields are populated and after committing the transaction a domain event is published via the producer.  When listing entities, the service returns a tuple of items and total count; pagination is performed in SQL.  When updating and deleting, the service raises HTTP errors when records do not exist or belong to another tenant.  The `patch_my_entity` function applies an array of JSON Patch operations in order, updates only the changed fields and emits an updated event if anything actually changed.  The private helper `_apply_my_entity_patch_operation` dispatches logic for each patch operation type (add, remove, replace, copy, move, test) and path.  New domain services should follow this pattern: accept the tenant identifier, perform DB operations within a transaction and publish events only after a successful commit.

### `app/messaging/producers`

Producers publish domain events to the message broker via Celery.  `my_entity_producer.py` defines a `MyEntityProducer` class with three class methods: `send_my_entity_created`, `send_my_entity_updated` and `send_my_entity_deleted`.  Each method constructs a Pydantic message model (`MyEntityCreatedMessage`, etc.), builds message headers containing the tenant and entity identifiers, wraps the domain message in an `EventEnvelope`, sets correlation and causation identifiers using context variables, and invokes `celery_app.send_task` with an explicit task name (which doubles as the routing key).  The task names follow the convention `conversa.<domain>.<event>`.  If you add a new domain, you should implement a similar producer class that defines constants for its task names, builds appropriate message models and sends tasks using the shared Celery application.  The only service‑specific changes in this module are the task names and the message schemas; everything else should remain as written.

### `app/messaging/tasks`

Tasks are Celery functions that consume events.  `my_entity_tasks.py` registers three tasks with names matching the producer’s routing keys (`conversa.my-entity.created`, `conversa.my-entity.updated` and `conversa.my-entity.deleted`).  Each task accepts either an `envelope` (containing a full `EventEnvelope`) or a legacy `payload` dictionary.  A helper `_parse_envelope` normalises these inputs into an `EventEnvelope`.  Trace and correlation identifiers are propagated into the current span via `_propagate_trace`.  The tasks validate the domain payload using the event schemas, log structured messages containing key identifiers and, for updates, include the list of changed fields.  By default the tasks do not perform any side effects, but in a real service you might update projection tables or trigger downstream workflows here.  Tasks enable automatic retries with exponential backoff and jitter by setting `autoretry_for`, `retry_backoff`, `retry_jitter` and `max_retries` on the decorator.  When adding a new domain, replicate these patterns: define tasks whose names match the producer’s routing keys, parse and validate the envelope, propagate trace context and implement your side effects.

### `tests/api` and `tests/domain/services`

The test suite verifies both the API and service layers.  `tests/api/test_health.py` checks that the liveness and readiness endpoints return the expected `HealthResponse` and HTTP status codes.  `tests/api/test_my_entity.py` uses dummy sessions and monkeypatching to ensure that each API route correctly invokes the corresponding service function, passes the right parameters (including the current user) and wraps responses in the expected Pydantic models.  `tests/domain/services/test_my_entity_service.py` exercises the service layer directly using a real PostgreSQL instance provisioned by `docker-compose.test.yml`.  It covers happy paths (creating records, listing them with pagination, retrieving existing entities, updating with changes and without changes, applying JSON Patch operations and deleting) as well as error cases (missing or mismatched tenant IDs and simulated database failures).  The tests also verify that events are published with correct payloads by monkeypatching the producer methods.  When adding a new domain you should copy the test files and adjust the domain name; use the existing tests as a template for verifying your business logic and API wiring.

---

## Boilerplate code

Many files in this repository provide infrastructure that is reused across domains and services.  These files are considered **boilerplate**, meaning they should be copied as‑is when creating a new service.  Only service names, environment variables or task names should be adjusted where explicitly noted.  The following components are boilerplate:

* **`app/core/celery_app.py`** – Defines the shared Celery application, configures the broker and result backend, declares a topic exchange (`conversa`) and a dead‑letter exchange, sets up default queues and routes for the `my_entity` domain and automatically discovers tasks under `app.messaging`.  When copying to a new service you must update the Celery application name (`my-entity-service`), define new queues and routes for additional domains and adjust routing keys.  All other settings (JSON serialisation, UTC timestamps, retry on start‑up) should remain untouched.
* **`app/core/telemetry.py`** – Encapsulates OpenTelemetry instrumentation.  Functions `init_tracing`, `instrument_fastapi`, `instrument_httpx`, `instrument_sqlalchemy` and `instrument_celery` configure tracing for the API and worker processes.  The module degrades gracefully when instrumentation packages are unavailable.  This file should be copied verbatim; only the `service_name` passed to `init_tracing` in your entry points (`main_api.py` and `celery_app.py`) changes.
* **`app/core/logging.py`** – Sets up JSON logging with a custom `JsonLogFormatter` that enriches logs with trace identifiers and user‑provided extras.  The `configure_logging` function binds the formatter to the root logger; it is called at the top of `main_api.py` and by the Celery application.  Do not modify the logging configuration; consistent structured logs across services is a key requirement for observability.
* **`app/core/db.py`** – Provides lazy initialisation of the SQLAlchemy engine and sessionmaker.  The engine is created only on first use to allow tests to override `DATABASE_URL` at runtime.  Helper functions `get_db` (FastAPI dependency) and `get_cm_db` (context manager for non‑API code) yield sessions with proper cleanup.  The module explicitly **must not** call `Base.metadata.create_all()`; Liquibase owns the schema.  When copying this file, ensure that you import your models so SQLAlchemy knows about them, but otherwise leave the implementation unchanged.
* **`app/core/config.py`** – Centralises configuration.  It reads environment variables for database connections, Liquibase properties, JWT secrets, Auth0 domain and algorithms, Celery broker and result backend, API keys (e.g. for external services) and optional tokens for IP lookup services.  Defaults are provided for development.  When creating a new service update the default database names, usernames and environment variable names to reflect the new service; do not remove existing methods as other parts of the code rely on them.
* **`app/api/error_handlers.py`** – Registers global exception handlers on a FastAPI app to ensure that errors are returned with consistent JSON bodies (`ErrorResponseBody`).  It handles `HTTPException`, `ValidationError` and generic `Exception` by logging the error and returning structured error responses.  This pattern should be reused verbatim in all services.
* **`app/domain/models/base.py`** – Defines the declarative base used by SQLAlchemy.  It sets naming conventions for constraints so that generated constraint names are stable.  Copy this file without modification.
* **Common schemas** – `app/domain/schemas/common.py` and `app/domain/schemas/health.py` provide reusable models for error responses, pagination and health checks.  These should be copied as‑is.
* **JWT utilities** – `app/util/jwt_util.py` implements a dependency factory `auth_jwt` to enforce required claims on incoming requests, as well as a helper `generate_test_jwt` for unit tests.  The `auth_jwt` dependency decodes the token using the configured secret and algorithm, verifies required claims (e.g. tenant_id) and returns the top‑level claims plus any custom claims.  When adding a new service you may need to update claim names, but the general structure should remain consistent.
* **Correlation utilities** – `app/util/correlation.py` manages correlation and causation identifiers via context variables.  The functions `get_correlation_id`, `set_correlation_id`, `get_message_id` and `set_message_id` store values that propagate across asynchronous calls.  They also attach attributes to OpenTelemetry spans when available.  Do not modify this behaviour.
* **Liquibase helper** – `app/util/liquibase.py` wraps the `pyliquibase` library and exposes `apply_changelog` to validate and apply database migrations using a properties file.  The default property file is `migrations/liquibase/docker-liquibase.properties`.  When copying the starter, keep this wrapper and adjust only the path to the property file if you reorganise your migrations directory.
* **AI agent placeholder** – `app/ai/agents/simple_agent.py` contains a minimal echo agent used to demonstrate AI integration.  It simply returns the prompt it receives.  This file is optional when adding a new domain; it exists to illustrate how one might encapsulate AI functionality in a dedicated module.
* **Packaging metadata** – `app/formless_agent_service.egg-info/` holds Python package metadata generated by `poetry` or `pip`.  These files are not edited manually; they are regenerated when packaging the project.
* **Design directory** – `design/` is intentionally empty in this starter.  Teams can add architecture diagrams or design documents here; it is not used by the service runtime.
* **Telemetry configuration tests** – `tests/test_otel_collector_config.py` and `tests/test_telemetry_resource_merge.py` verify that the OpenTelemetry collector configuration file is valid YAML and that telemetry resources merge correctly.  These tests ensure observability remains intact when the service is extended; copy them unchanged.

---

## Core infrastructure deep dive

The starter includes several core modules that underpin the service infrastructure.  Understanding their responsibilities helps prevent misuse when adding new features.

### Celery application (`app/core/celery_app.py`)

The Celery module instantiates a single, shared Celery application named `my-entity-service`.  It configures the broker and result backend using environment variables exposed via `Config`, ensures that tasks and results are serialised as JSON, enforces UTC timestamps and retries connecting to the broker on start‑up.  The configuration declares a **topic exchange** (`conversa`) and a dead‑letter exchange (`conversa.dlx`).  Three queues are defined: a default catch‑all queue, a `conversa.my-entity` queue for domain events and a dead‑letter queue for messages that cannot be processed.  Routing rules map task names to queues and routing keys so that created, updated and deleted events for `MyEntity` are delivered to the correct queue.  Finally, the module automatically discovers tasks under `app.messaging` and initialises OpenTelemetry instrumentation for Celery and HTTPX.  When adapting this file, change the application name and add new queues and routes for each new domain; do not remove the instrumentation or default configuration.

### Telemetry (`app/core/telemetry.py`)

Telemetry is handled centrally to provide consistent distributed tracing.  The `init_tracing` function configures an OpenTelemetry tracer provider with an OTLP exporter.  It reads the service name from the environment (`OTEL_SERVICE_NAME`) or falls back to the value passed by the caller.  Additional functions instrument FastAPI applications, Celery workers, HTTPX clients and SQLAlchemy engines.  When called, these functions wrap the respective libraries so that spans are automatically created around HTTP requests, database queries and task execution.  Telemetry gracefully degrades when instrumentation packages are not installed.  You should not modify this module; instead, ensure that `main_api.py` and `celery_app.py` call `init_tracing` and the relevant instrumentation functions with the appropriate service names (e.g. `my-entity-service` for the API and `my-entity-worker` for the worker).

### Logging (`app/core/logging.py`)

Structured logging is critical for observability.  The logging module defines a `JsonLogFormatter` that serialises log records to JSON with fields such as `timestamp`, `level`, `name`, `message`, `trace_id`, `span_id`, `request_id`, `entrypoint` and `task_name`.  It preserves any custom attributes attached via the `extra` argument.  The `configure_logging` function creates a single `StreamHandler` with this formatter and attaches it to the root logger.  It reads the `LOG_LEVEL` environment variable to set the logging level.  A backward‑compatibility alias `setup_logging` is provided.  All services should call `configure_logging` once at import time to ensure that log messages include trace context.  Do not change the logging structure or format.

### Database initialisation (`app/core/db.py`)

The database layer lazily initialises a SQLAlchemy engine and sessionmaker based on the current `DATABASE_URL` environment variable.  This allows test suites to override the database URL without reloading the module.  A global lock ensures thread safety when initialising.  The helper `get_db` is a FastAPI dependency that yields a session per request and closes it afterwards; `get_cm_db` provides a context manager for non‑API code such as Celery tasks.  The function `check_database_connection` executes a simple `SELECT 1` to verify readiness and is used by the readiness probe.  The module imports the `MyEntity` model on import to register it with SQLAlchemy but does not create the table.  When adding new models, import them here so SQLAlchemy knows about them.  Never call `Base.metadata.create_all()`; the schema is owned by Liquibase.

### Configuration (`app/core/config.py`)

Configuration values are centralised in the `Config` class.  Each static method reads an environment variable and falls back to a sensible development default.  The configuration covers the SQLAlchemy database URL, Liquibase enablement and property file path, JWT secrets and algorithms, Auth0 domain, Celery broker URL and result backend, API keys for external services and tokens for IP geolocation.  All values are returned as strings except `liquibase_enabled()` which interprets truthy values (`true`, `1`, `yes`, `y`) as `True`.  When copying the starter you should adjust the default database names (e.g. from `my_entity_service` to `your_service_name`), user names and passwords in both the defaults and the `docker/postgres/init-database.sql` script.  Environment variables override these defaults at runtime.

---

## Database layer

### PostgreSQL initialisation script (`docker/postgres/init-database.sql`)

The initialisation script creates the main PostgreSQL database and users.  It runs automatically when the `db` container starts.  The script performs the following actions:

* Creates a database named `my_entity_service`.
* Connects to that database and creates three users: `my_entity_app` (used by the API service), `my_entity_admin` (used by Liquibase migrations) and `my_entity_worker` (used by the Celery worker).  Each user is assigned a password.
* Creates the `public` schema owned by `my_entity_admin` and grants privileges.  The `my_entity_app` user receives usage and CRUD privileges on tables and usage on sequences; the `my_entity_worker` user receives usage and read/write privileges but cannot delete rows by default.

When creating a new service you must update the database name and user names in this script to reflect your service.  The Docker compose file references the same credentials when wiring services to the database.  Do not alter the privilege model unless you have a specific requirement; isolating API and worker permissions is considered a best practice.

### Liquibase migrations

Liquibase is the authoritative source of truth for the database schema.  The root changelog (`migrations/liquibase/changelog-root.xml`) includes all SQL migration files in `migrations/liquibase/sql/` via the `<includeAll>` directive.  In this starter there is a single migration file, `001_create_my_entity.sql`, which creates the `my_entity` table, defines its columns and adds an index on `tenant_id`.  When adding new domain tables, create additional numbered SQL files in the `sql` directory (e.g. `002_create_your_domain.sql`) and Liquibase will apply them in lexical order.  SQL change sets should be idempotent and include comments describing their purpose.

Liquibase runs in two contexts:

* **At service startup.** `main_api.py` calls `apply_changelog` from `app.util.liquibase` during the application lifespan.  This will validate the database schema and apply pending migrations when `LIQUIBASE_ENABLED=true` (default).  You can disable this behaviour in development by setting `LIQUIBASE_ENABLED=false`.
* **Manually via command line.** The `apply_changelog` function can be invoked directly (`python -m app.util.liquibase apply_changelog`) and accepts a `property_file` argument.  Three property files are provided: `docker-liquibase.properties` (used in Docker deployments), `liquibase.properties` (used for local development) and `test-liquibase.properties` (used by the integration tests).  These files define the JDBC URL, credentials, changelog file and location of the PostgreSQL JDBC driver.  They mirror the values provided in `docker/postgres/init-database.sql` and `docker-compose.yml`.  When you rename your service or change database credentials, update these property files accordingly.

It is critical that **all code obeys the schema defined by Liquibase**.  Do not add or modify columns in the SQLAlchemy models without first creating a migration.  The schema is the gold standard that all other layers must follow.

---

## Docker architecture

### Dockerfiles

Two Dockerfiles build the API and worker images.  Both share a multi‑stage pattern where a base image installs system dependencies (`build-essential`, `libpq-dev` and optionally `default-jdk-headless` for Liquibase), a second stage installs Python dependencies defined in `pyproject.toml` and a final stage copies the full source code.  The API image (`docker/api/Dockerfile`) exposes port 8000 and runs `uvicorn main_api:app`.  The worker image (`docker/worker/Dockerfile`) runs the Celery worker using the shared application (`-A app.core.celery_app.celery_app`) and logs at `info` level.  These files should not be redesigned; update only the image names or base versions if necessary.  When copying to a new service, adjust the names of the images and ensure the service name in `celery_app.py` matches the expected environment variables.

### Docker compose files

The primary `docker-compose.yml` orchestrates all components required to run the service in development or in a containerised deployment.  It defines the following services:

* **`db`** – A PostgreSQL 15 container.  The `container_name` is `my_entity_db`.  It loads `docker/postgres/init-database.sql` at start‑up to create the database and users.  A health check (`pg_isready`) waits until the database is ready.  When creating a new service you should change only the `container_name` and, if you rename the database or users, adjust the environment variables and volumes accordingly.
* **`rabbitmq`** – A RabbitMQ 4.2.1 container with the management plugin.  The `container_name` is `my_entity_rabbitmq`.  Environment variables set the default user, password and virtual host to `my_entity`.  Ports 5672 (AMQP) and 15672 (management UI) are exposed.  Only the `container_name` should change when you rename the service; the user/password should align with those in `init-database.sql` and `Config.celery_broker_url()`.
* **`otel-collector`** – An OpenTelemetry collector based on the `otel/opentelemetry-collector-contrib:0.97.0` image.  It mounts `docker/otel/otel-collector-config.yml` and exposes ports 14318 (OTLP HTTP) and 14317 (OTLP gRPC).  It depends on the `jaeger` service and forwards traces to Jaeger.  This service is mandatory and should be copied without modifications.
* **`jaeger`** – Jaeger all‑in‑one collector and UI.  It exposes port 16686 for the UI and ports 24317/24318 for OTLP receivers.  Environment variables enable the OTLP receiver.  Copy this service verbatim; it is essential for tracing.
* **`my-entity-service`** – Builds the API image using `docker/api/Dockerfile`.  It is named `my_entity_service_app` and exposes port 8080 externally (mapped to 8000 in the container).  Environment variables configure the database URL (pointing to the `my_entity_app` user), enable Liquibase migrations, configure Celery broker and result backend and set OpenTelemetry exporter options.  The service depends on the database, RabbitMQ and the collector.  When creating a new service update the `container_name`, `image` build context (if you rename the directory), environment variables (database names, usernames, service names) and port mappings.
* **`my-entity-worker`** – Builds the worker image using `docker/worker/Dockerfile`.  It is named `my_entity_worker`.  Environment variables mirror those of the API (database URL and broker configuration) but the service name for OpenTelemetry is `my-entity-worker`.  Dependencies mirror the API.  Update the `container_name` and service names when creating a new service.

The file also defines volumes for persistent database and RabbitMQ data.  Do not modify the structure of `docker-compose.yml`; only the names and credentials should change when deriving a new service.

The `docker-compose.test.yml` file is used exclusively by the test suite to provision a PostgreSQL instance on a different port (25432).  It defines a `db` service similar to the primary compose file but omits RabbitMQ, Jaeger and other services.  Integration tests rely on this file to spin up a disposable database.  When extending the service you should not change this file except for the `container_name` if you rename the service.

### OpenTelemetry collector configuration (`docker/otel/otel-collector-config.yml`)

This YAML file configures the OpenTelemetry collector.  It defines an OTLP receiver over HTTP and gRPC, a batch processor and an OTLP exporter pointing to the `jaeger` container’s OTLP gRPC endpoint.  TLS verification is disabled because Jaeger’s OTLP receiver does not expose TLS by default.  Traces emitted by the API and worker are forwarded through this collector to Jaeger.  Do not modify this file other than to change exporter endpoints if your tracing infrastructure differs; the test suite validates its structure.

---

## Tests and supporting files

The `tests/` directory contains pytest modules that exercise API endpoints, service functions and telemetry configuration.  The `conftest.py` file defines fixtures used across test modules, such as database sessions and HTTP clients.  `test_otel_collector_config.py` ensures that the OpenTelemetry collector configuration file is valid YAML and contains expected keys.  `test_telemetry_resource_merge.py` verifies that telemetry resources merge correctly when service names are overridden.  Under `ai/` a simple agent test confirms that the `SimpleAgent` echoes input.  The `api/` tests ensure that the API routes invoke the service layer correctly and respect the current user.  The `domain/services/` tests exercise the business logic and event publishing.  All tests are meant to be copied and adapted when you add new domains; they provide examples of how to set up test contexts and use monkeypatching to isolate units under test.

---

## Extending the starter for a new domain

To add a new domain to your service or to generate an entirely new microservice from this starter, follow these steps.  The process is deterministic: for each area listed you should copy the existing `MyEntity` implementation, rename it to your new domain and adjust the schema and business logic.

1. **Define your database schema.**  Create a new SQL file under `migrations/liquibase/sql/` with the next sequential number (e.g. `002_create_your_entity.sql`).  Write the `CREATE TABLE` statements, indexes and constraints for your domain.  Update or add Liquibase changelog entries as necessary.  Liquibase will apply this migration on service start.
2. **Create a SQLAlchemy model.**  In `app/domain/models/`, copy `my_entity.py` to `<your_entity>.py`, set `__tablename__` to the name of your table and add mapped columns corresponding to your schema.  Import the model in `app/core/db.py` to register it with SQLAlchemy.
3. **Define Pydantic schemas.**  In `app/domain/schemas/`, copy `my_entity.py` to `<your_entity>.py` and adjust the fields for your domain.  Update the base class names (`MyEntityBase` → `YourEntityBase`, etc.) and descriptions.  If your resource supports JSON Patch, update `MY_ENTITY_JSON_PATCH_ALLOWED_PATH_PREFIXES` accordingly.  Create corresponding event message models in `app/domain/schemas/events/<your_entity>_events.py` and update `EventEnvelope` if additional metadata is needed.
4. **Implement service functions.**  Copy `my_entity_service.py` to `<your_entity>_service.py` and rename functions (`create_my_entity` → `create_your_entity`, etc.).  Adjust queries, default audit fields and event publishing as needed.  Ensure each function accepts `tenant_id` and a database session and raises appropriate HTTP exceptions on errors.
5. **Expose API routes.**  Copy `app/api/routes/my_entity.py` to `app/api/routes/<your_entity>.py`.  Change the router prefix (`/tenants/{tenant_id}/my-entities` → `/tenants/{tenant_id}/your-entities`), rename the dependency injection types and adjust responses.  Import your router in `main_api.py` and call `app.include_router(...)`.
6. **Publish and consume events.**  Create a producer class in `app/messaging/producers/<your_entity>_producer.py` with task names like `conversa.your-entity.created`.  Create Celery tasks in `app/messaging/tasks/<your_entity>_tasks.py` that handle those events.  Add corresponding queues and routes in `app/core/celery_app.py` so that messages are delivered to the correct handlers.
7. **Write tests.**  Duplicate the API and service tests (`test_my_entity.py` and `test_my_entity_service.py`) and update them for your domain.  Add integration tests for new behaviours.  Ensure that your migrations are applied in `tests/postgres` by using `pytest.mark.liquibase` and that your service layer publishes events correctly.
8. **Update configuration and Docker.**  Change the database names, usernames and passwords in `docker/postgres/init-database.sql`, `Config.database_url()` defaults and the environment variables in `docker-compose.yml`.  Update the Celery application name and service names in `celery_app.py`, `docker-compose.yml` and `main_api.py`.

By following these steps and adhering to the patterns described in this README, you can produce a new microservice that is tenant‑aware, event‑driven and observability‑ready.  Avoid inventing new patterns; the strength of the starter lies in its consistency.  Always ensure that your code aligns with the database schema defined in Liquibase and that you copy boilerplate code without modification except where explicitly instructed.

---

## Summary

`my_entity_starter_python` is a comprehensive, opinionated template for building microservices.  It balances completeness with minimalism by implementing a single domain end‑to‑end while demonstrating production‑ready patterns: FastAPI routing, SQLAlchemy models, Liquibase migrations, Celery messaging, OpenTelemetry tracing, structured logging, rigorous tests and container orchestration.  This README enumerates every file, explains its purpose and clearly distinguishes between boilerplate and domain‑specific code.  When you derive new services from this starter, follow the documented patterns exactly, update identifiers where necessary and avoid inventing new architectures.  Doing so ensures that your service is maintainable, observable and consistent with the established best practices embodied in this repository.